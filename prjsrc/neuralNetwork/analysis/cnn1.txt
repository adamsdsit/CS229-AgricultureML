C:\Users\bohdan\miniconda3\envs\cs229\python.exe C:/Users/bohdan/Documents/Stanford/CS229-34/Project/prjsrc/neuralNetwork/cnn.py
./train_images/
(1, 500, 500, 3)
(2, 500, 500, 3)
(3, 500, 500, 3)
(4, 500, 500, 3)
(5, 500, 500, 3)
(6, 500, 500, 3)
(7, 500, 500, 3)
(8, 500, 500, 3)
(9, 500, 500, 3)
(10, 500, 500, 3)
(11, 500, 500, 3)
(12, 500, 500, 3)
(13, 500, 500, 3)
(14, 500, 500, 3)
(15, 500, 500, 3)
(16, 500, 500, 3)
(17, 500, 500, 3)
(18, 500, 500, 3)
(19, 500, 500, 3)
(20, 500, 500, 3)
(21, 500, 500, 3)
(22, 500, 500, 3)
(23, 500, 500, 3)
(24, 500, 500, 3)
(25, 500, 500, 3)
(26, 500, 500, 3)
(27, 500, 500, 3)
(28, 500, 500, 3)
(29, 500, 500, 3)
(30, 500, 500, 3)
(31, 500, 500, 3)
(32, 500, 500, 3)
(33, 500, 500, 3)
(34, 500, 500, 3)
(35, 500, 500, 3)
(36, 500, 500, 3)
(37, 500, 500, 3)
(38, 500, 500, 3)
(39, 500, 500, 3)
(40, 500, 500, 3)
(41, 500, 500, 3)
(42, 500, 500, 3)
(43, 500, 500, 3)
(44, 500, 500, 3)
(45, 500, 500, 3)
(46, 500, 500, 3)
(47, 500, 500, 3)
(48, 500, 500, 3)
(49, 500, 500, 3)
(50, 500, 500, 3)
(51, 500, 500, 3)
(52, 500, 500, 3)
(53, 500, 500, 3)
(54, 500, 500, 3)
(55, 500, 500, 3)
(56, 500, 500, 3)
(57, 500, 500, 3)
(58, 500, 500, 3)
(59, 500, 500, 3)
(60, 500, 500, 3)
(61, 500, 500, 3)
(62, 500, 500, 3)
(63, 500, 500, 3)
(64, 500, 500, 3)
(65, 500, 500, 3)
(66, 500, 500, 3)
(67, 500, 500, 3)
(68, 500, 500, 3)
(69, 500, 500, 3)
(70, 500, 500, 3)
(71, 500, 500, 3)
(72, 500, 500, 3)
(73, 500, 500, 3)
(74, 500, 500, 3)
(75, 500, 500, 3)
(76, 500, 500, 3)
(77, 500, 500, 3)
(78, 500, 500, 3)
(79, 500, 500, 3)
(80, 500, 500, 3)
(81, 500, 500, 3)
(82, 500, 500, 3)
(83, 500, 500, 3)
(84, 500, 500, 3)
(85, 500, 500, 3)
(86, 500, 500, 3)
(87, 500, 500, 3)
(88, 500, 500, 3)
(89, 500, 500, 3)
(90, 500, 500, 3)
(91, 500, 500, 3)
(92, 500, 500, 3)
(93, 500, 500, 3)
(94, 500, 500, 3)
(95, 500, 500, 3)
(96, 500, 500, 3)
(97, 500, 500, 3)
(98, 500, 500, 3)
(99, 500, 500, 3)
(100, 500, 500, 3)
(101, 500, 500, 3)
(102, 500, 500, 3)
(103, 500, 500, 3)
(104, 500, 500, 3)
(105, 500, 500, 3)
(106, 500, 500, 3)
(107, 500, 500, 3)
(108, 500, 500, 3)
(109, 500, 500, 3)
(110, 500, 500, 3)
(111, 500, 500, 3)
(112, 500, 500, 3)
(113, 500, 500, 3)
(114, 500, 500, 3)
(115, 500, 500, 3)
(116, 500, 500, 3)
(117, 500, 500, 3)
(118, 500, 500, 3)
(119, 500, 500, 3)
(120, 500, 500, 3)
(121, 500, 500, 3)
(122, 500, 500, 3)
(123, 500, 500, 3)
(124, 500, 500, 3)
(125, 500, 500, 3)
(126, 500, 500, 3)
(127, 500, 500, 3)
(128, 500, 500, 3)
(129, 500, 500, 3)
(130, 500, 500, 3)
(131, 500, 500, 3)
(132, 500, 500, 3)
(133, 500, 500, 3)
(134, 500, 500, 3)
(135, 500, 500, 3)
(136, 500, 500, 3)
(137, 500, 500, 3)
(138, 500, 500, 3)
(139, 500, 500, 3)
(140, 500, 500, 3)
(141, 500, 500, 3)
(142, 500, 500, 3)
(143, 500, 500, 3)
(144, 500, 500, 3)
(145, 500, 500, 3)
(146, 500, 500, 3)
(147, 500, 500, 3)
(148, 500, 500, 3)
(149, 500, 500, 3)
(150, 500, 500, 3)
(151, 500, 500, 3)
(152, 500, 500, 3)
(153, 500, 500, 3)
(154, 500, 500, 3)
(155, 500, 500, 3)
(156, 500, 500, 3)
(157, 500, 500, 3)
(158, 500, 500, 3)
(159, 500, 500, 3)
(160, 500, 500, 3)
(161, 500, 500, 3)
(162, 500, 500, 3)
(163, 500, 500, 3)
(164, 500, 500, 3)
(165, 500, 500, 3)
(166, 500, 500, 3)
(167, 500, 500, 3)
(168, 500, 500, 3)
(169, 500, 500, 3)
(170, 500, 500, 3)
(171, 500, 500, 3)
(172, 500, 500, 3)
(173, 500, 500, 3)
(174, 500, 500, 3)
(175, 500, 500, 3)
(176, 500, 500, 3)
(177, 500, 500, 3)
(178, 500, 500, 3)
(179, 500, 500, 3)
(180, 500, 500, 3)
(181, 500, 500, 3)
(182, 500, 500, 3)
(183, 500, 500, 3)
(184, 500, 500, 3)
(185, 500, 500, 3)
(186, 500, 500, 3)
(187, 500, 500, 3)
(188, 500, 500, 3)
(189, 500, 500, 3)
(190, 500, 500, 3)
(191, 500, 500, 3)
(192, 500, 500, 3)
(193, 500, 500, 3)
(194, 500, 500, 3)
(195, 500, 500, 3)
(196, 500, 500, 3)
(197, 500, 500, 3)
(198, 500, 500, 3)
(199, 500, 500, 3)
(200, 500, 500, 3)
(201, 500, 500, 3)
(202, 500, 500, 3)
(203, 500, 500, 3)
(204, 500, 500, 3)
(205, 500, 500, 3)
(206, 500, 500, 3)
(207, 500, 500, 3)
(208, 500, 500, 3)
(209, 500, 500, 3)
(210, 500, 500, 3)
(211, 500, 500, 3)
(212, 500, 500, 3)
(213, 500, 500, 3)
(214, 500, 500, 3)
(215, 500, 500, 3)
(216, 500, 500, 3)
(217, 500, 500, 3)
(218, 500, 500, 3)
(219, 500, 500, 3)
(220, 500, 500, 3)
(221, 500, 500, 3)
(222, 500, 500, 3)
(223, 500, 500, 3)
(224, 500, 500, 3)
(225, 500, 500, 3)
(226, 500, 500, 3)
(227, 500, 500, 3)
(228, 500, 500, 3)
(229, 500, 500, 3)
(230, 500, 500, 3)
(231, 500, 500, 3)
(232, 500, 500, 3)
(233, 500, 500, 3)
(234, 500, 500, 3)
(235, 500, 500, 3)
(236, 500, 500, 3)
(237, 500, 500, 3)
(238, 500, 500, 3)
(239, 500, 500, 3)
(240, 500, 500, 3)
(241, 500, 500, 3)
(242, 500, 500, 3)
(243, 500, 500, 3)
(244, 500, 500, 3)
(245, 500, 500, 3)
(246, 500, 500, 3)
(247, 500, 500, 3)
(248, 500, 500, 3)
(249, 500, 500, 3)
(250, 500, 500, 3)
(251, 500, 500, 3)
(252, 500, 500, 3)
(253, 500, 500, 3)
(254, 500, 500, 3)
(255, 500, 500, 3)
(256, 500, 500, 3)
(257, 500, 500, 3)
(258, 500, 500, 3)
(259, 500, 500, 3)
(260, 500, 500, 3)
(261, 500, 500, 3)
(262, 500, 500, 3)
(263, 500, 500, 3)
(264, 500, 500, 3)
(265, 500, 500, 3)
(266, 500, 500, 3)
(267, 500, 500, 3)
(268, 500, 500, 3)
(269, 500, 500, 3)
(270, 500, 500, 3)
(271, 500, 500, 3)
(272, 500, 500, 3)
(273, 500, 500, 3)
(274, 500, 500, 3)
(275, 500, 500, 3)
(276, 500, 500, 3)
(277, 500, 500, 3)
(278, 500, 500, 3)
(279, 500, 500, 3)
(280, 500, 500, 3)
(281, 500, 500, 3)
(282, 500, 500, 3)
(283, 500, 500, 3)
(284, 500, 500, 3)
(285, 500, 500, 3)
(286, 500, 500, 3)
(287, 500, 500, 3)
(288, 500, 500, 3)
(289, 500, 500, 3)
(290, 500, 500, 3)
(291, 500, 500, 3)
(292, 500, 500, 3)
(293, 500, 500, 3)
(294, 500, 500, 3)
(295, 500, 500, 3)
(296, 500, 500, 3)
(297, 500, 500, 3)
(298, 500, 500, 3)
(299, 500, 500, 3)
(300, 500, 500, 3)
(301, 500, 500, 3)
(302, 500, 500, 3)
(303, 500, 500, 3)
(304, 500, 500, 3)
(305, 500, 500, 3)
(306, 500, 500, 3)
(307, 500, 500, 3)
(308, 500, 500, 3)
(309, 500, 500, 3)
(310, 500, 500, 3)
(311, 500, 500, 3)
(312, 500, 500, 3)
(313, 500, 500, 3)
(314, 500, 500, 3)
(315, 500, 500, 3)
(316, 500, 500, 3)
(317, 500, 500, 3)
(318, 500, 500, 3)
(319, 500, 500, 3)
(320, 500, 500, 3)
(321, 500, 500, 3)
(322, 500, 500, 3)
(323, 500, 500, 3)
(324, 500, 500, 3)
(325, 500, 500, 3)
(326, 500, 500, 3)
(327, 500, 500, 3)
(328, 500, 500, 3)
(329, 500, 500, 3)
(330, 500, 500, 3)
(331, 500, 500, 3)
(332, 500, 500, 3)
(333, 500, 500, 3)
(334, 500, 500, 3)
(335, 500, 500, 3)
(336, 500, 500, 3)
(337, 500, 500, 3)
(338, 500, 500, 3)
(339, 500, 500, 3)
(340, 500, 500, 3)
(341, 500, 500, 3)
(342, 500, 500, 3)
(343, 500, 500, 3)
(344, 500, 500, 3)
(345, 500, 500, 3)
(346, 500, 500, 3)
(347, 500, 500, 3)
(348, 500, 500, 3)
(349, 500, 500, 3)
(350, 500, 500, 3)
(351, 500, 500, 3)
(352, 500, 500, 3)
(353, 500, 500, 3)
(354, 500, 500, 3)
(355, 500, 500, 3)
(356, 500, 500, 3)
(357, 500, 500, 3)
(358, 500, 500, 3)
(359, 500, 500, 3)
(360, 500, 500, 3)
(361, 500, 500, 3)
(362, 500, 500, 3)
(363, 500, 500, 3)
(364, 500, 500, 3)
(365, 500, 500, 3)
(366, 500, 500, 3)
(367, 500, 500, 3)
(368, 500, 500, 3)
(369, 500, 500, 3)
(370, 500, 500, 3)
(371, 500, 500, 3)
(372, 500, 500, 3)
(373, 500, 500, 3)
(374, 500, 500, 3)
(375, 500, 500, 3)
(376, 500, 500, 3)
(377, 500, 500, 3)
(378, 500, 500, 3)
(379, 500, 500, 3)
(380, 500, 500, 3)
(381, 500, 500, 3)
(382, 500, 500, 3)
(383, 500, 500, 3)
(384, 500, 500, 3)
(385, 500, 500, 3)
(386, 500, 500, 3)
(387, 500, 500, 3)
(388, 500, 500, 3)
(389, 500, 500, 3)
(390, 500, 500, 3)
(391, 500, 500, 3)
(392, 500, 500, 3)
(393, 500, 500, 3)
(394, 500, 500, 3)
(395, 500, 500, 3)
(396, 500, 500, 3)
(397, 500, 500, 3)
(398, 500, 500, 3)
(399, 500, 500, 3)
(400, 500, 500, 3)
(401, 500, 500, 3)
(402, 500, 500, 3)
(403, 500, 500, 3)
(404, 500, 500, 3)
(405, 500, 500, 3)
(406, 500, 500, 3)
(407, 500, 500, 3)
(408, 500, 500, 3)
(409, 500, 500, 3)
(410, 500, 500, 3)
(411, 500, 500, 3)
(412, 500, 500, 3)
(413, 500, 500, 3)
(414, 500, 500, 3)
(415, 500, 500, 3)
(416, 500, 500, 3)
(417, 500, 500, 3)
(418, 500, 500, 3)
(419, 500, 500, 3)
(420, 500, 500, 3)
(421, 500, 500, 3)
(422, 500, 500, 3)
(423, 500, 500, 3)
(424, 500, 500, 3)
(425, 500, 500, 3)
(426, 500, 500, 3)
(427, 500, 500, 3)
(428, 500, 500, 3)
(429, 500, 500, 3)
(430, 500, 500, 3)
(431, 500, 500, 3)
(432, 500, 500, 3)
(433, 500, 500, 3)
(434, 500, 500, 3)
(435, 500, 500, 3)
(436, 500, 500, 3)
(437, 500, 500, 3)
(438, 500, 500, 3)
(439, 500, 500, 3)
(440, 500, 500, 3)
(441, 500, 500, 3)
(442, 500, 500, 3)
(443, 500, 500, 3)
(444, 500, 500, 3)
(445, 500, 500, 3)
(446, 500, 500, 3)
(447, 500, 500, 3)
(448, 500, 500, 3)
(449, 500, 500, 3)
(450, 500, 500, 3)
(451, 500, 500, 3)
(452, 500, 500, 3)
(453, 500, 500, 3)
(454, 500, 500, 3)
(455, 500, 500, 3)
./test_images/
(1, 500, 500, 3)
(2, 500, 500, 3)
(3, 500, 500, 3)
(4, 500, 500, 3)
(5, 500, 500, 3)
(6, 500, 500, 3)
(7, 500, 500, 3)
(8, 500, 500, 3)
(9, 500, 500, 3)
(10, 500, 500, 3)
(11, 500, 500, 3)
(12, 500, 500, 3)
(13, 500, 500, 3)
(14, 500, 500, 3)
(15, 500, 500, 3)
(16, 500, 500, 3)
(17, 500, 500, 3)
(18, 500, 500, 3)
(19, 500, 500, 3)
(20, 500, 500, 3)
(21, 500, 500, 3)
(22, 500, 500, 3)
(23, 500, 500, 3)
(24, 500, 500, 3)
(25, 500, 500, 3)
(26, 500, 500, 3)
(27, 500, 500, 3)
(28, 500, 500, 3)
(29, 500, 500, 3)
(30, 500, 500, 3)
(31, 500, 500, 3)
(32, 500, 500, 3)
(33, 500, 500, 3)
(34, 500, 500, 3)
(35, 500, 500, 3)
(36, 500, 500, 3)
(37, 500, 500, 3)
(38, 500, 500, 3)
(39, 500, 500, 3)
(40, 500, 500, 3)
(41, 500, 500, 3)
(42, 500, 500, 3)
(43, 500, 500, 3)
(44, 500, 500, 3)
(45, 500, 500, 3)
(46, 500, 500, 3)
(47, 500, 500, 3)
(48, 500, 500, 3)
(49, 500, 500, 3)
(50, 500, 500, 3)
(51, 500, 500, 3)
(52, 500, 500, 3)
(53, 500, 500, 3)
(54, 500, 500, 3)
(55, 500, 500, 3)
(56, 500, 500, 3)
(57, 500, 500, 3)
(58, 500, 500, 3)
(59, 500, 500, 3)
(60, 500, 500, 3)
(61, 500, 500, 3)
(62, 500, 500, 3)
(63, 500, 500, 3)
(64, 500, 500, 3)
(65, 500, 500, 3)
(66, 500, 500, 3)
(67, 500, 500, 3)
(68, 500, 500, 3)
(69, 500, 500, 3)
(70, 500, 500, 3)
(71, 500, 500, 3)
(72, 500, 500, 3)
(73, 500, 500, 3)
(74, 500, 500, 3)
(75, 500, 500, 3)
(76, 500, 500, 3)
(77, 500, 500, 3)
(78, 500, 500, 3)
(79, 500, 500, 3)
(80, 500, 500, 3)
(81, 500, 500, 3)
(82, 500, 500, 3)
(83, 500, 500, 3)
(84, 500, 500, 3)
(85, 500, 500, 3)
(86, 500, 500, 3)
(87, 500, 500, 3)
(88, 500, 500, 3)
(89, 500, 500, 3)
(90, 500, 500, 3)
(91, 500, 500, 3)
number of training examples = 455
number of test examples = 91
X_train shape: (455, 500, 500, 3)
Y_train shape: (455, 10)
X_test shape: (91, 500, 500, 3)
Y_test shape: (91, 10)
2021-11-22 15:30:49.161119: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 500, 500, 3)]     0         
_________________________________________________________________
conv2d (Conv2D)              (None, 500, 500, 64)      12352     
_________________________________________________________________
re_lu (ReLU)                 (None, 500, 500, 64)      0         
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 63, 63, 64)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 63, 63, 32)        32800     
_________________________________________________________________
re_lu_1 (ReLU)               (None, 63, 63, 32)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
flatten (Flatten)            (None, 8192)              0         
_________________________________________________________________
dense (Dense)                (None, 10)                81930     
=================================================================
Total params: 127,082
Trainable params: 127,082
Non-trainable params: 0
_________________________________________________________________
Train for 8 steps, validate for 2 steps
Epoch 1/100
2021-11-22 15:30:49.363131: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2730000000 exceeds 10% of system memory.
2021-11-22 15:30:52.723323: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 4096000000 exceeds 10% of system memory.
2021-11-22 15:31:40.917080: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 4096000000 exceeds 10% of system memory.

1/8 [==>...........................] - ETA: 22:51 - loss: 2.2739 - accuracy: 0.0000e+002021-11-22 15:34:05.539352: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 4096000000 exceeds 10% of system memory.
2021-11-22 15:34:41.664418: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 4096000000 exceeds 10% of system memory.

2/8 [======>.......................] - ETA: 17:36 - loss: 2.5115 - accuracy: 0.0391    
3/8 [==========>...................] - ETA: 14:04 - loss: 2.4352 - accuracy: 0.0260
4/8 [==============>...............] - ETA: 10:59 - loss: 2.3090 - accuracy: 0.1094
5/8 [=================>............] - ETA: 7:55 - loss: 2.2464 - accuracy: 0.1500 
6/8 [=====================>........] - ETA: 5:27 - loss: 2.5259 - accuracy: 0.1250
7/8 [=========================>....] - ETA: 3:08 - loss: 2.6122 - accuracy: 0.1071
8/8 [==============================] - 1510s 189s/step - loss: 2.6003 - accuracy: 0.1055 - val_loss: 2.1905 - val_accuracy: 0.2088
Epoch 2/100

1/8 [==>...........................] - ETA: 40:07 - loss: 2.2755 - accuracy: 0.0625
2/8 [======>.......................] - ETA: 36:44 - loss: 2.2524 - accuracy: 0.0703
3/8 [==========>...................] - ETA: 29:01 - loss: 2.2850 - accuracy: 0.0469
4/8 [==============>...............] - ETA: 21:22 - loss: 2.2738 - accuracy: 0.0547
5/8 [=================>............] - ETA: 16:23 - loss: 2.2681 - accuracy: 0.0437
6/8 [=====================>........] - ETA: 10:02 - loss: 2.2556 - accuracy: 0.0365
7/8 [=========================>....] - ETA: 4:34 - loss: 2.2357 - accuracy: 0.0312 
8/8 [==============================] - 1970s 246s/step - loss: 2.1906 - accuracy: 0.0308 - val_loss: 1.9367 - val_accuracy: 0.2088
Epoch 3/100

1/8 [==>...........................] - ETA: 16:05 - loss: 1.8974 - accuracy: 0.3906
2/8 [======>.......................] - ETA: 13:48 - loss: 2.0500 - accuracy: 0.2344
3/8 [==========>...................] - ETA: 16:06 - loss: 2.2692 - accuracy: 0.1562
4/8 [==============>...............] - ETA: 14:01 - loss: 2.3176 - accuracy: 0.1367
5/8 [=================>............] - ETA: 10:49 - loss: 2.3427 - accuracy: 0.1094
6/8 [=====================>........] - ETA: 7:08 - loss: 2.2781 - accuracy: 0.1667 
7/8 [=========================>....] - ETA: 3:45 - loss: 2.2291 - accuracy: 0.1786
8/8 [==============================] - 1678s 210s/step - loss: 2.1875 - accuracy: 0.1802 - val_loss: 2.0148 - val_accuracy: 0.1758
Epoch 4/100

1/8 [==>...........................] - ETA: 26:21 - loss: 2.0525 - accuracy: 0.3594
2/8 [======>.......................] - ETA: 22:46 - loss: 1.9865 - accuracy: 0.2188
3/8 [==========>...................] - ETA: 17:29 - loss: 1.9946 - accuracy: 0.1458
4/8 [==============>...............] - ETA: 14:09 - loss: 1.9940 - accuracy: 0.1289
5/8 [=================>............] - ETA: 10:31 - loss: 2.0329 - accuracy: 0.1656
6/8 [=====================>........] - ETA: 7:47 - loss: 2.0428 - accuracy: 0.1380 
7/8 [=========================>....] - ETA: 3:49 - loss: 2.0366 - accuracy: 0.1183
8/8 [==============================] - 1716s 215s/step - loss: 2.0026 - accuracy: 0.1165 - val_loss: 1.9921 - val_accuracy: 0.1758
Epoch 5/100

1/8 [==>...........................] - ETA: 34:11 - loss: 2.1174 - accuracy: 0.3438
2/8 [======>.......................] - ETA: 30:20 - loss: 1.9799 - accuracy: 0.2109
3/8 [==========>...................] - ETA: 24:14 - loss: 2.0047 - accuracy: 0.1406
4/8 [==============>...............] - ETA: 17:29 - loss: 2.0171 - accuracy: 0.1250
5/8 [=================>............] - ETA: 12:17 - loss: 2.0787 - accuracy: 0.1000
6/8 [=====================>........] - ETA: 7:42 - loss: 2.0627 - accuracy: 0.1589 
7/8 [=========================>....] - ETA: 3:39 - loss: 2.0328 - accuracy: 0.1942
8/8 [==============================] - 1645s 206s/step - loss: 1.9917 - accuracy: 0.2022 - val_loss: 1.9511 - val_accuracy: 0.2088
Epoch 6/100

1/8 [==>...........................] - ETA: 21:15 - loss: 1.9994 - accuracy: 0.3906
2/8 [======>.......................] - ETA: 18:13 - loss: 1.9594 - accuracy: 0.2344
3/8 [==========>...................] - ETA: 17:35 - loss: 2.0072 - accuracy: 0.1562
4/8 [==============>...............] - ETA: 15:06 - loss: 2.0292 - accuracy: 0.1367
5/8 [=================>............] - ETA: 11:55 - loss: 2.0806 - accuracy: 0.1094
6/8 [=====================>........] - ETA: 7:39 - loss: 2.0527 - accuracy: 0.1667 
7/8 [=========================>....] - ETA: 3:57 - loss: 2.0195 - accuracy: 0.2009
8/8 [==============================] - 1785s 223s/step - loss: 1.9800 - accuracy: 0.2088 - val_loss: 1.9393 - val_accuracy: 0.2088
Epoch 7/100

1/8 [==>...........................] - ETA: 45:11 - loss: 1.9664 - accuracy: 0.3906
2/8 [======>.......................] - ETA: 35:01 - loss: 1.9480 - accuracy: 0.2344
3/8 [==========>...................] - ETA: 28:34 - loss: 2.0155 - accuracy: 0.1562
4/8 [==============>...............] - ETA: 21:14 - loss: 2.0463 - accuracy: 0.1367
5/8 [=================>............] - ETA: 14:30 - loss: 2.1078 - accuracy: 0.1094
6/8 [=====================>........] - ETA: 9:52 - loss: 2.0731 - accuracy: 0.1667 
7/8 [=========================>....] - ETA: 4:52 - loss: 2.0335 - accuracy: 0.2009
8/8 [==============================] - 2145s 268s/step - loss: 1.9889 - accuracy: 0.2088 - val_loss: 1.9398 - val_accuracy: 0.2088
Epoch 8/100

1/8 [==>...........................] - ETA: 24:17 - loss: 1.9818 - accuracy: 0.3906
2/8 [======>.......................] - ETA: 17:45 - loss: 1.9417 - accuracy: 0.2344
3/8 [==========>...................] - ETA: 13:42 - loss: 1.9997 - accuracy: 0.1562
4/8 [==============>...............] - ETA: 10:08 - loss: 2.0252 - accuracy: 0.1367
5/8 [=================>............] - ETA: 7:11 - loss: 2.0826 - accuracy: 0.1094 
6/8 [=====================>........] - ETA: 4:37 - loss: 2.0560 - accuracy: 0.1667
7/8 [=========================>....] - ETA: 2:18 - loss: 2.0233 - accuracy: 0.2009
8/8 [==============================] - 1021s 128s/step - loss: 1.9829 - accuracy: 0.2088 - val_loss: 1.9468 - val_accuracy: 0.2088
Epoch 9/100

1/8 [==>...........................] - ETA: 16:05 - loss: 1.9938 - accuracy: 0.3906
2/8 [======>.......................] - ETA: 16:47 - loss: 1.9429 - accuracy: 0.2344
3/8 [==========>...................] - ETA: 12:37 - loss: 1.9899 - accuracy: 0.1562
4/8 [==============>...............] - ETA: 9:30 - loss: 2.0113 - accuracy: 0.1367 
5/8 [=================>............] - ETA: 6:48 - loss: 2.0672 - accuracy: 0.1094
6/8 [=====================>........] - ETA: 4:23 - loss: 2.0447 - accuracy: 0.1667
7/8 [=========================>....] - ETA: 2:08 - loss: 2.0143 - accuracy: 0.2009
8/8 [==============================] - 967s 121s/step - loss: 1.9732 - accuracy: 0.2088 - val_loss: 1.9465 - val_accuracy: 0.2088
Epoch 10/100

1/8 [==>...........................] - ETA: 28:22 - loss: 1.9936 - accuracy: 0.3906
2/8 [======>.......................] - ETA: 22:17 - loss: 1.9419 - accuracy: 0.2344
3/8 [==========>...................] - ETA: 20:19 - loss: 1.9917 - accuracy: 0.1562
4/8 [==============>...............] - ETA: 17:24 - loss: 2.0148 - accuracy: 0.1367